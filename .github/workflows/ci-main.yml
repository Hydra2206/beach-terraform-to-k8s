name: CI Main - Apply + Build + Deploy

on:
  workflow_dispatch: {}
  push:
    branches:
      - main

permissions:
  contents: read
  id-token: write

env:
  TERRAFORM_DIR: ./terraform
  APP_DIR: ./app

jobs:
  terraform-apply:
    name: Terraform Apply (plan + apply) — emits apply_status output
    runs-on: ubuntu-latest
    outputs:
      apply_status: ${{ steps.tf_apply.outputs.apply_status }}
      kubeconfig: ${{ steps.write-kubeconfig.outputs.kubeconfig-path }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Show runner path (debug)
        run: |
          echo "Working dir: $(pwd)"
          ls -la

      - name: Fetch Terraform vars from AWS Secrets Manager
        id: fetch_secret
        run: |
          echo "Retrieving secret from Secrets Manager..."
          # get secret string (plain JSON). will fail the step if it cannot retrieve.
          SECRET_JSON=$(aws secretsmanager get-secret-value --secret-id "${{ secrets.AWS_TF_VARS_SECRET_ARN }}" --query SecretString --output text)
          if [ -z "$SECRET_JSON" ]; then
            echo "::error ::Failed to read secret or secret is empty"
            exit 1
          fi
          # mask secret to avoid leaking in logs (best-effort)
          echo "::add-mask::$SECRET_JSON"
          # write JSON into terraform folder as terraform.tfvars.json so Terraform auto-loads it
          mkdir -p ./terraform
          echo "$SECRET_JSON" > ./terraform/terraform.tfvars.json
          echo "Wrote terraform/terraform.tfvars.json (not printed here for security)"

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Terraform validate
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform validate || true

      - name: Terraform plan (non-interactive)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform plan -out=tfplan.binary -input=false || true

      - name: Terraform Apply (auto-approve)
        id: tf_apply
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          # Try apply and capture result; always exit 0 so job doesn't fail early.
          set +e
          terraform apply -auto-approve tfplan.binary
          RC=$?
          if [ "$RC" -eq 0 ]; then
            echo "apply_status=success" >> "$GITHUB_OUTPUT"
            echo "apply_rc=0" >> "$GITHUB_OUTPUT"
          else
            echo "apply_status=failure" >> "$GITHUB_OUTPUT"
            echo "apply_rc=${RC}" >> "$GITHUB_OUTPUT"
          fi
          # Keep the step itself exit 0 so job completes and outputs are visible
          exit 0

      - name: Output kubeconfig to file (if apply succeeded)
        id: write-kubeconfig
        run: |
          # Default to empty kubeconfig output
          echo "kubeconfig-path=" >> "$GITHUB_OUTPUT"

          if [ "${{ steps.tf_apply.outputs.apply_status }}" = "success" ]; then
            mkdir -p $GITHUB_WORKSPACE/.kube
            # If terraform outputs are present, update kubeconfig
            # Silence errors so we still set an output (if missing, remains blank)
            set +e
            CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "")
            CLUSTER_REGION=$(terraform output -raw cluster_region 2>/dev/null || echo "${{ secrets.AWS_REGION }}")
            if [ -n "$CLUSTER_NAME" ]; then
              aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$CLUSTER_REGION" --kubeconfig $GITHUB_WORKSPACE/.kube/config 2>/dev/null || true
              if [ -f $GITHUB_WORKSPACE/.kube/config ]; then
                echo "kubeconfig-path=$GITHUB_WORKSPACE/.kube/config" >> "$GITHUB_OUTPUT"
              fi
            fi
            set -e
          fi
        working-directory: ${{ env.TERRAFORM_DIR }}

  build-and-deploy:
    name: Build & Deploy (runs only if terraform apply succeeded)
    needs: terraform-apply
    runs-on: ubuntu-latest
    if: needs.terraform-apply.outputs.apply_status == 'success'
    environment: production
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Debug repo layout
        run: |
          echo "GITHUB_WORKSPACE=$GITHUB_WORKSPACE"
          pwd
          ls -la
          echo "Check ./app"
          if [ -d "./app" ]; then ls -la ./app; else echo "NO ./app folder"; fi

      - name: Configure AWS credentials to push to ECR
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: ./app
          push: true
          tags: ${{ steps.ecr-login.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}

      - name: Set KUBECONFIG
        run: |
          mkdir -p $HOME/.kube

          if [ -z "${{ needs.terraform-apply.outputs.kubeconfig }}" ]; then
          echo "Kubeconfig is empty — skipping deploy."
          exit 0
          fi
          
          # copy kubeconfig created by previous job
          cp ${{ needs.terraform-apply.outputs.kubeconfig }} $HOME/.kube/config
          export KUBECONFIG=$HOME/.kube/config
        shell: bash

      - name: Deploy to EKS (kubectl apply)
        run: |
          # Replace image reference in deployment manifest or use `kubectl set image`
          sed -i "s|IMAGE_PLACEHOLDER|${{ steps.ecr-login.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}|g" ./app/k8s/deployment.yaml
          kubectl apply -f ./app/k8s

  destroy-on-failure:
    name: Terraform Destroy (runs only if terraform apply failed)
    needs: terraform-apply
    runs-on: ubuntu-latest
    if: needs.terraform-apply.outputs.apply_status == 'failure'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials via OIDC (for destroy)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Show runner path (debug)
        run: |
          echo "Working dir: $(pwd)"
          ls -la
      - name: Fetch Terraform vars from AWS Secrets Manager
        id: fetch_secret
        run: |
          echo "Retrieving secret from Secrets Manager..."
          # get secret string (plain JSON). will fail the step if it cannot retrieve.
          SECRET_JSON=$(aws secretsmanager get-secret-value --secret-id "${{ secrets.AWS_TF_VARS_SECRET_ARN }}" --query SecretString --output text)
          if [ -z "$SECRET_JSON" ]; then
            echo "::error ::Failed to read secret or secret is empty"
            exit 1
          fi
          # mask secret to avoid leaking in logs (best-effort)
          echo "::add-mask::$SECRET_JSON"
          # write JSON into terraform folder as terraform.tfvars.json so Terraform auto-loads it
          mkdir -p ./terraform
          echo "$SECRET_JSON" > ./terraform/terraform.tfvars.json
          echo "Wrote terraform/terraform.tfvars.json (not printed here for security)"

      - name: Terraform init (for destroy)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Terraform refresh state (best-effort)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform refresh -input=false || true

      - name: Terraform Destroy (cleanup)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          echo "Terraform apply failed; attempting terraform destroy to clean up resources..."
          set -e
          terraform destroy -auto-approve
