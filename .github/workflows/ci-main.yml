name: CI Main - Apply + Build + Deploy

on:
  workflow_dispatch: {}
  push:
    branches:
      - main

permissions:
  contents: read         #allows actions to read repo contents
  id-token: write

env:
  TERRAFORM_DIR: ./terraform
  APP_DIR: ./app

jobs:
  terraform-apply:
    name: Terraform Apply (plan + apply) â€” emits apply_status output
    runs-on: ubuntu-latest
    outputs:                  # exposes job-level outputs that other jobs can reference as needs.terraform-apply.outputs.*
      apply_status: ${{ steps.tf_apply.outputs.apply_status }}
      kubeconfig: ${{ steps.write-kubeconfig.outputs.kubeconfig-path }}
      cluster_name: ${{ steps.export-tf.outputs.cluster_name }}
      cluster_region: ${{ steps.export-tf.outputs.cluster_region }}
    
    steps:
      - name: Checkout   # Downloads your repo to the runner VM.
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}


      - name: Fetch Terraform vars from AWS Secrets Manager
        id: fetch_secret
        run: |
          echo "Retrieving secret from Secrets Manager..."
          # get secret string (plain JSON). will fail the step if it cannot retrieve.
          SECRET_JSON=$(aws secretsmanager get-secret-value --secret-id "${{ secrets.AWS_TF_VARS_SECRET_ARN }}" --query SecretString --output text)
          if [ -z "$SECRET_JSON" ]; then
            echo "::error ::Failed to read secret or secret is empty"
            exit 1
          fi
          # mask secret to avoid leaking in logs (best-effort)
          echo "::add-mask::$SECRET_JSON"
          # write JSON into terraform folder as terraform.tfvars.json so Terraform auto-loads it
          mkdir -p ./terraform
          echo "$SECRET_JSON" > ./terraform/terraform.tfvars.json
          echo "Wrote terraform/terraform.tfvars.json (not printed here for security)"

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false         #-input=false prevents interactive prompts.

      - name: Terraform validate
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform validate || true      # || true ensures the step returns success even if terraform validate fails (so the job continues).

      - name: Terraform plan
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform plan -out=tfplan.binary -input=false || true     # Creates a plan and writes it to tfplan.binary

      - name: Terraform Apply (auto-approve)
        id: tf_apply
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          # Try apply and capture result; always exit 0 so job doesn't fail early.
          set +e        # disables exit-on-error so the script can inspect the return code instead of failing the step immediately.
          terraform apply -auto-approve tfplan.binary
          RC=$?
          if [ "$RC" -eq 0 ]; then
            echo "apply_status=success" >> "$GITHUB_OUTPUT"
            echo "apply_rc=0" >> "$GITHUB_OUTPUT"
          else
            echo "apply_status=failure" >> "$GITHUB_OUTPUT"
            echo "apply_rc=${RC}" >> "$GITHUB_OUTPUT"
          fi
          # Keep the step itself exit 0 so job completes and outputs are visible
          exit 0
      
      - name: Generate kubeconfig                     # you do need a kubeconfig file to interact with the cluster after it exists.
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          mkdir -p "$GITHUB_WORKSPACE/.kube"
          aws eks update-kubeconfig \
            --name "${{ secrets.CLUSTER_NAME }}" \
            --region "${{ secrets.AWS_REGION }}" \
            --kubeconfig "$GITHUB_WORKSPACE/.kube/config"

      - name: Debug kubeconfig path
        run: |
          pwd
          ls -R .
      
      - name: Upload kubeconfig           # Uploads the kubeconfig as a workflow artifact so other jobs can download it.
        uses: actions/upload-artifact@v4
        with:
          name: kubeconfig
          path: ./.kube/config

  build-and-deploy:
    name: Build & Deploy (runs only if terraform apply succeeded)
    needs: terraform-apply
    runs-on: ubuntu-latest
    if: needs.terraform-apply.outputs.apply_status == 'success'
    environment: production
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials to push to ECR
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and Push Docker Image
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
          IMAGE_TAG: v1
        run: |
          IMAGE_URI=${REGISTRY}/${REPOSITORY}:${IMAGE_TAG}
          docker build -t $IMAGE_URI ./app
          docker push $IMAGE_URI

      - name: Download kubeconfig
        uses: actions/download-artifact@v4
        with:
          name: kubeconfig
          path: ./.kube

      - name: Export kubeconfig         # Adds KUBECONFIG environment variable to runner's environment so kubectl uses that config.
        run: |
          echo "KUBECONFIG=$GITHUB_WORKSPACE/.kube/config" >> $GITHUB_ENV

      - name: Update deployment image
        env:
          REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
          IMAGE_TAG: v1
        run: |
          IMAGE_URI=${REGISTRY}/${REPOSITORY}:${IMAGE_TAG}
          sed -i "s|IMAGE_PLACEHOLDER|$IMAGE_URI|g" ./app/K8s/deployment.yaml

      - name: Deploy to EKS
        run: |
          kubectl apply -f ./app/K8s/deployment.yaml
          kubectl apply -f ./app/K8s/service.yaml

  destroy-on-failure:
    name: Terraform Destroy (runs only if terraform apply failed)
    needs: terraform-apply
    runs-on: ubuntu-latest
    if: needs.terraform-apply.outputs.apply_status == 'failure'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials via OIDC (for destroy)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Show runner path (debug)
        run: |
          echo "Working dir: $(pwd)"
          ls -la
      - name: Fetch Terraform vars from AWS Secrets Manager
        id: fetch_secret
        run: |
          echo "Retrieving secret from Secrets Manager..."
          # get secret string (plain JSON). will fail the step if it cannot retrieve.
          SECRET_JSON=$(aws secretsmanager get-secret-value --secret-id "${{ secrets.AWS_TF_VARS_SECRET_ARN }}" --query SecretString --output text)
          if [ -z "$SECRET_JSON" ]; then
            echo "::error ::Failed to read secret or secret is empty"
            exit 1
          fi
          # mask secret to avoid leaking in logs (best-effort)
          echo "::add-mask::$SECRET_JSON"
          # write JSON into terraform folder as terraform.tfvars.json so Terraform auto-loads it
          mkdir -p ./terraform
          echo "$SECRET_JSON" > ./terraform/terraform.tfvars.json
          echo "Wrote terraform/terraform.tfvars.json (not printed here for security)"

      - name: Terraform init (for destroy)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Terraform refresh state (best-effort)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform refresh -input=false || true

      - name: Terraform Destroy (cleanup)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          echo "Terraform apply failed; attempting terraform destroy to clean up resources..."
          set -e
          terraform destroy -auto-approve
