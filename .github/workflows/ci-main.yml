name: CI Main - Apply + Build + Deploy

on:
  workflow_dispatch: {}
  push:
    branches:
      - main

permissions:
  contents: read
  id-token: write

env:
  TERRAFORM_DIR: ./terraform
  APP_DIR: ./app

jobs:
  terraform-apply:
    name: Terraform Apply (plan + apply) — emits apply_status output
    runs-on: ubuntu-latest
    outputs:
      apply_status: ${{ steps.tf_apply.outputs.apply_status }}
      kubeconfig: ${{ steps.write-kubeconfig.outputs.kubeconfig-path }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Show runner path (debug)
        run: |
          echo "Working dir: $(pwd)"
          ls -la

      - name: Fetch Terraform vars from AWS Secrets Manager
        id: fetch_secret
        run: |
          echo "Retrieving secret from Secrets Manager..."
          # get secret string (plain JSON). will fail the step if it cannot retrieve.
          SECRET_JSON=$(aws secretsmanager get-secret-value --secret-id "${{ secrets.AWS_TF_VARS_SECRET_ARN }}" --query SecretString --output text)
          if [ -z "$SECRET_JSON" ]; then
            echo "::error ::Failed to read secret or secret is empty"
            exit 1
          fi
          # mask secret to avoid leaking in logs (best-effort)
          echo "::add-mask::$SECRET_JSON"
          # write JSON into terraform folder as terraform.tfvars.json so Terraform auto-loads it
          mkdir -p ./terraform
          echo "$SECRET_JSON" > ./terraform/terraform.tfvars.json
          echo "Wrote terraform/terraform.tfvars.json (not printed here for security)"

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Terraform validate
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform validate || true

      - name: Terraform plan (non-interactive)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform plan -out=tfplan.binary -input=false || true

      - name: Terraform Apply (auto-approve)
        id: tf_apply
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          # Try apply and capture result; always exit 0 so job doesn't fail early.
          set +e
          terraform apply -auto-approve tfplan.binary
          RC=$?
          if [ "$RC" -eq 0 ]; then
            echo "apply_status=success" >> "$GITHUB_OUTPUT"
            echo "apply_rc=0" >> "$GITHUB_OUTPUT"
          else
            echo "apply_status=failure" >> "$GITHUB_OUTPUT"
            echo "apply_rc=${RC}" >> "$GITHUB_OUTPUT"
          fi
          # Keep the step itself exit 0 so job completes and outputs are visible
          exit 0

      - name: Output kubeconfig to file (if apply succeeded)
        id: write-kubeconfig
        run: |
          # Default to empty kubeconfig output
          echo "kubeconfig-path=" >> "$GITHUB_OUTPUT"

          if [ "${{ steps.tf_apply.outputs.apply_status }}" = "success" ]; then
            mkdir -p $GITHUB_WORKSPACE/.kube
            # If terraform outputs are present, update kubeconfig
            # Silence errors so we still set an output (if missing, remains blank)
            set +e
            CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "")
            CLUSTER_REGION=$(terraform output -raw cluster_region 2>/dev/null || echo "${{ secrets.AWS_REGION }}")
            if [ -n "$CLUSTER_NAME" ]; then
              aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$CLUSTER_REGION" --kubeconfig $GITHUB_WORKSPACE/.kube/config 2>/dev/null || true
              if [ -f $GITHUB_WORKSPACE/.kube/config ]; then
                echo "kubeconfig-path=$GITHUB_WORKSPACE/.kube/config" >> "$GITHUB_OUTPUT"
              fi
            fi
            set -e
          fi
        working-directory: ${{ env.TERRAFORM_DIR }}

  build-and-deploy:
    name: Build & Deploy (runs only if terraform apply succeeded)
    needs: terraform-apply
    runs-on: ubuntu-latest
    if: needs.terraform-apply.outputs.apply_status == 'success'
    environment: production
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials to push to ECR
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: ./app
          push: true
          tags: ${{ steps.ecr-login.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}

      - name: Set KUBECONFIG (create/update, wait for API)
        id: set-kube
        run: |
          set -euo pipefail
          mkdir -p "$HOME/.kube"

          # Guard: if no kubeconfig path was produced by terraform job, skip safely
          if [ -z "${{ needs.terraform-apply.outputs.kubeconfig }}" ]; then
            echo "Kubeconfig output is empty — skipping deploy."
            echo "kube_ready=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Copy kubeconfig produced by terraform job (it should be a full path)
          SRC_KUBECONFIG="${{ needs.terraform-apply.outputs.kubeconfig }}"
          if [ ! -f "$SRC_KUBECONFIG" ]; then
            echo "::error ::Kubeconfig file not found at '$SRC_KUBECONFIG' (maybe terraform didn't write it)"
            echo "kube_ready=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          cp "$SRC_KUBECONFIG" "$HOME/.kube/config"
          export KUBECONFIG="$HOME/.kube/config"
          echo "Wrote kubeconfig to $KUBECONFIG"

          # Ensure aws cli can reach cluster endpoint (update-kubeconfig is idempotent, re-run to be safe)
          # Use Terraform outputs to find cluster name/region if available (silently fallback to secret region)
          CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "")
          CLUSTER_REGION=$(terraform output -raw cluster_region 2>/dev/null || echo "${{ secrets.AWS_REGION }}")

          if [ -n "$CLUSTER_NAME" ]; then
            aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$CLUSTER_REGION" --kubeconfig "$KUBECONFIG" || true
          fi

          # Wait for the API server to become available (max ~5 minutes)
          echo "Waiting for Kubernetes API to be reachable..."
          for i in $(seq 1 30); do
          if kubectl version --short >/dev/null 2>&1; then
            echo "Kubernetes API is reachable"
            echo "kube_ready=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi
            echo "API not ready yet ($i/30). Sleeping 10s..."
            sleep 10
          done  

          echo "::warning ::Kubernetes API did not become reachable in time"
          echo "kube_ready=false" >> "$GITHUB_OUTPUT"
        working-directory: ${{ env.TERRAFORM_DIR }}
        shell: bash

      - name: Deploy to EKS (kubectl apply)
        if: steps.set-kube.outputs.kube_ready == 'true'
        run: |
          set -euo pipefail
          export KUBECONFIG="$HOME/.kube/config"

          # Replace image placeholder in manifest(s)
          sed -i "s|IMAGE_PLACEHOLDER|${{ steps.ecr-login.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}|g" ./app/K8s/deployment.yaml || true

          # Apply manifests. Use --validate=false to skip client-side openapi validation if needed.
          kubectl apply -f ./app/K8s --validate=false
          shell: bash

      - name: Deploy to EKS (kubectl apply)
        run: |
          # Replace image reference in deployment manifest or use `kubectl set image`
          sed -i "s|IMAGE_PLACEHOLDER|${{ steps.ecr-login.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}|g" ./app/K8s/deployment.yaml
          kubectl apply -f ./app/K8s

  destroy-on-failure:
    name: Terraform Destroy (runs only if terraform apply failed)
    needs: terraform-apply
    runs-on: ubuntu-latest
    if: needs.terraform-apply.outputs.apply_status == 'failure'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials via OIDC (for destroy)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Show runner path (debug)
        run: |
          echo "Working dir: $(pwd)"
          ls -la
      - name: Fetch Terraform vars from AWS Secrets Manager
        id: fetch_secret
        run: |
          echo "Retrieving secret from Secrets Manager..."
          # get secret string (plain JSON). will fail the step if it cannot retrieve.
          SECRET_JSON=$(aws secretsmanager get-secret-value --secret-id "${{ secrets.AWS_TF_VARS_SECRET_ARN }}" --query SecretString --output text)
          if [ -z "$SECRET_JSON" ]; then
            echo "::error ::Failed to read secret or secret is empty"
            exit 1
          fi
          # mask secret to avoid leaking in logs (best-effort)
          echo "::add-mask::$SECRET_JSON"
          # write JSON into terraform folder as terraform.tfvars.json so Terraform auto-loads it
          mkdir -p ./terraform
          echo "$SECRET_JSON" > ./terraform/terraform.tfvars.json
          echo "Wrote terraform/terraform.tfvars.json (not printed here for security)"

      - name: Terraform init (for destroy)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform init -input=false

      - name: Terraform refresh state (best-effort)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform refresh -input=false || true

      - name: Terraform Destroy (cleanup)
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          echo "Terraform apply failed; attempting terraform destroy to clean up resources..."
          set -e
          terraform destroy -auto-approve
